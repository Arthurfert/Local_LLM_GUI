# Local LLM GUI - Interface Ollama avec PySide6

Une interface graphique moderne et intuitive pour interagir avec des modÃ¨les de langage locaux via Ollama.

## ğŸ“‹ PrÃ©requis

- Python 3.8 ou supÃ©rieur
- Ollama installÃ© et en cours d'exÃ©cution ([Installation Ollama](https://ollama.ai))
- Un ou plusieurs modÃ¨les Ollama tÃ©lÃ©chargÃ©s

## ğŸš€ Installation

1. **Cloner le repository**
```powershell
git clone https://github.com/Arthurfert/Local_LLM_GUI.git
cd Local_LLM_GUI
```

2. **CrÃ©er un environnement virtuel (recommandÃ©)**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

3. **Installer les dÃ©pendances**
```powershell
pip install -r requirements.txt
```

4. **Installer et lancer Ollama**

TÃ©lÃ©chargez Ollama depuis [ollama.ai](https://ollama.ai) et installez-le.

TÃ©lÃ©chargez un modÃ¨le (exemple avec llama2) :
```powershell
ollama pull llama2
```

Autres modÃ¨les populaires :
- `ollama pull mistral` - Mistral 7B
- `ollama pull codellama` - CodeLlama pour la programmation
- `ollama pull llama3` - Llama 3

## ğŸ’» Utilisation

1. **Assurez-vous qu'Ollama est en cours d'exÃ©cution**

Ollama devrait se lancer automatiquement. VÃ©rifiez avec :
```powershell
ollama list
```

2. **Lancer l'application**
```powershell
python main.py
```

3. **Utiliser l'interface**
   - SÃ©lectionnez un modÃ¨le dans la liste dÃ©roulante
   - Tapez votre message dans la zone de texte
   - Cliquez sur "ğŸ“¤ Envoyer" ou appuyez sur Ctrl+EntrÃ©e
   - Les rÃ©ponses s'affichent dans la zone de conversation
   - Utilisez "ğŸ—‘ï¸ Effacer" pour rÃ©initialiser la conversation

## ğŸ¨ FonctionnalitÃ©s

- âœ… Interface graphique moderne avec PySide6
- âœ… SÃ©lection dynamique des modÃ¨les installÃ©s
- âœ… Historique de conversation
- âœ… Traitement asynchrone (l'interface ne se fige pas)
- âœ… Gestion des erreurs
- âœ… Support de plusieurs modÃ¨les Ollama
- âœ… RafraÃ®chissement de la liste des modÃ¨les

## ğŸ“ Structure du projet

```
Local_LLM_GUI/
â”œâ”€â”€ main.py                 # Point d'entrÃ©e de l'application
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main_window.py     # FenÃªtre principale
â””â”€â”€ core/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ ollama_client.py   # Client API Ollama
```

## ğŸ”§ Configuration

Par dÃ©faut, l'application se connecte Ã  Ollama sur `http://localhost:11434`.

Pour modifier l'URL d'Ollama, Ã©ditez le fichier `core/ollama_client.py` :
```python
def __init__(self, base_url="http://localhost:11434"):
```

## ğŸ› DÃ©pannage

**ProblÃ¨me : "Aucun modÃ¨le disponible"**
- VÃ©rifiez qu'Ollama est en cours d'exÃ©cution : `ollama list`
- TÃ©lÃ©chargez un modÃ¨le : `ollama pull llama2`
- Cliquez sur le bouton "ğŸ”„ RafraÃ®chir"

**ProblÃ¨me : "Erreur de connexion"**
- Assurez-vous qu'Ollama est lancÃ©
- VÃ©rifiez que le port 11434 est accessible

**ProblÃ¨me : RÃ©ponse trÃ¨s lente**
- Les modÃ¨les LLM peuvent Ãªtre lents sur CPU
- Envisagez d'utiliser un modÃ¨le plus petit (ex: `ollama pull phi`)
- Fermez les autres applications gourmandes en ressources

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## ğŸ‘¨â€ğŸ’» Auteur

Arthur Fert
